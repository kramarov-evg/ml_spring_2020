
1st lab:
  1st subtask:
    [x] Read data @done(20-05-15 10:24)
    [x] Translate into floats @done(20-05-15 10:25)
    [x] Separate labels from features @done(20-05-15 10:26)
    [x] Try test/train split @started(20-05-15 10:58) @done(20-05-15 11:10) @lasted(12m9s)
    [x] Loop over test/train ratios @started(20-05-15 11:10) @done(20-05-15 11:27) @lasted(17m5s)
    [x] Refactor (logic to functions) @started(20-05-21 01:53) @done(20-05-21 02:55) @lasted(1h2m53s)
    [x] Plot results @done(20-05-21 02:55)
    [ ] Report on results
  2nd subtask:
    [x] Generate points @done(20-05-22 03:34)
    [x] Plot points @done(20-05-22 03:41)
    [ ] Adjust plot size @beautify
    [x] Create & train classifier @started(20-05-22 04:09) @done(20-05-22 04:37) @lasted(28m48s)
    [x] Get accuracy @done(20-05-22 04:48)
    [x] Confusion matrix @done(20-05-22 04:48)
    [x] ROC-curve @done(20-05-22 04:48)
    [x] PR-curve @done(20-05-22 04:48)
    [x] Plot data @done(20-05-22 04:48)
    [ ] Reimplement with splitting data
    [ ] Report on results
  3rd subtask:
    [x] Load data @started(20-05-22 10:15) @done(20-05-22 10:25) @lasted(10m13s)
    [x] Clean-up data @done(20-05-22 10:29)
    [x] Build classifier @started(20-05-22 11:47) @done(20-05-22 11:52) @lasted(5m44s)
    [x] Loop over neighbours count @started(20-05-22 11:52) @done(20-05-22 12:10) @lasted(18m45s)
    [x] Plot results @done(20-05-22 12:10)
    [x] Explore distance metrics @done(20-05-23 11:10)
    [x] Plot results @done(20-05-23 11:10)
    [x] Classify sample @done(20-05-23 11:10)
    [ ] Report on results
  4th subtask:
    [x] Read on SVM @done(20-05-23 18:40)
    [x] Load data @done(20-05-23 18:40)
    [x] Clean-up data @done(20-05-23 18:40)
    [x] Build classifier with linear kernel @done(20-05-23 18:40)
    [x] Train classifier & plot space division @done(20-05-23 18:41)
    [x] Plot confusion matrix (train + test) & print vectors number @done(20-05-23 18:41)
    [x] Change fine value to 0 error on training and test sets @done(20-05-23 18:41)
    [ ] Beautify @beautify
    4.e Overfitting happens only with rbf (gaussian) and is more visible on higher gamma values
    [ ] Unify 4.e
    [ ] Report on results
  5th subtask:
    [x] Load data @done(20-05-23 22:19)
    [x] Build base tree @done(20-05-23 22:50)
    [x] Visualize tree @done(20-05-23 22:50)
    [ ] Interpret results
    [ ] Decide whether tree is excessive
    [x] Experiment with params and test accuracy with them @done(20-05-24 01:06)
    [x] Load data @done(20-05-24 01:11)
    [x] Build optimal tree @done(20-05-24 01:42)
    [ ] Explain why is optimal
    Because not too big & good accuracy
    [x] Visualize tree @done(20-05-24 01:42)
    [x] Identify most important features @done(20-05-24 01:43)
    Most important is x1, second most is x0
    [x] Estimate classification accuracy @done(20-05-24 01:43)
  6th subtask:
    [x] Load data @done(20-05-24 02:23)
    [x] Create 2 classifiers @done(20-05-24 02:24)
    [x] Train them @done(20-05-24 02:24)
    [x] Select metrics @done(20-05-24 02:24)
    [x] Plot results @done(20-05-24 02:24)

2nd lab:
  1st subtask:
    [ ] Create neural network of a single neuron
    [ ] Train & evaluate on nn_0.csv & nn_1.csv. Explain why results differ
    [ ] How many epochs needed?
    [ ] Experiment with activation functions and optimizers
  2nd subtask:
    [ ] Modify NN from 1st task to fit optimally to nn_1.csv.
    [ ] What changed? Why?
  3rd subtask:
    [ ] Build classifier for MNIST dataset & evaluate classification quality
